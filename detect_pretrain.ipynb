{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghs1ge6PY-mQ"
      },
      "source": [
        "# Detecting Pretraining Data from Large Language Models\n",
        "\n",
        "This notebook implements the methods for detecting whether a piece of text was part of a language model's pretraining data. It includes functionality for:\n",
        "- Loading and preparing models\n",
        "- Calculating perplexity\n",
        "- Evaluating detection metrics\n",
        "- Visualizing results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/prahaladd/detect-pretrain-code.git"
      ],
      "metadata": {
        "id": "fxz5VmJHZcC3",
        "outputId": "f1ca579a-830b-4502-89d4-32278023b3fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'detect-pretrain-code'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (103/103), done.\u001b[K\n",
            "remote: Total 120 (delta 54), reused 56 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (120/120), 341.16 KiB | 8.12 MiB/s, done.\n",
            "Resolving deltas: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r detect-pretrain-code/src/requirements.txt"
      ],
      "metadata": {
        "id": "QG_EfzVwbiGe",
        "outputId": "efbaca22-736f-48f9-9a54-e4d98d7cb2da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 2)) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 3)) (4.51.3)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 4)) (4.67.1)\n",
            "Collecting datasets>=2.12.0 (from -r detect-pretrain-code/src/requirements.txt (line 5))\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: scikit-learn>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 7)) (1.6.1)\n",
            "Requirement already satisfied: openai>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from -r detect-pretrain-code/src/requirements.txt (line 8)) (1.76.0)\n",
            "Collecting zlib-wrapper>=0.1.3 (from -r detect-pretrain-code/src/requirements.txt (line 9))\n",
            "  Downloading zlib_wrapper-0.1.3.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ipdb>=0.13.0 (from -r detect-pretrain-code/src/requirements.txt (line 10))\n",
            "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (2.2.2)\n",
            "Collecting xxhash (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2))\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (3.11.15)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r detect-pretrain-code/src/requirements.txt (line 7)) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r detect-pretrain-code/src/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.0->-r detect-pretrain-code/src/requirements.txt (line 7)) (3.6.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: ipython>=7.31.1 in /usr/local/lib/python3.11/dist-packages (from ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (7.34.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (4.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (3.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (4.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai>=0.27.0->-r detect-pretrain-code/src/requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r detect-pretrain-code/src/requirements.txt (line 6)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r detect-pretrain-code/src/requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r detect-pretrain-code/src/requirements.txt (line 2)) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.12.0->-r detect-pretrain-code/src/requirements.txt (line 5)) (2025.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.31.1->ipdb>=0.13.0->-r detect-pretrain-code/src/requirements.txt (line 10)) (0.2.13)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: zlib-wrapper\n",
            "  Building wheel for zlib-wrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for zlib-wrapper: filename=zlib_wrapper-0.1.3-py3-none-any.whl size=4446 sha256=273677f773ea2f82ca00890fc5e22f415827a517f9f1a5bcabd8dd820d01e0ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/42/08/3d9dfb3650ac567e3115358e54f283c53fe1258ab7435a7f9f\n",
            "Successfully built zlib-wrapper\n",
            "Installing collected packages: zlib-wrapper, xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jedi, fsspec, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, ipdb, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 ipdb-0.13.13 jedi-0.19.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 xxhash-3.5.0 zlib-wrapper-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tixZ0fUQY-mS"
      },
      "source": [
        "# Import required libraries\n",
        "import logging\n",
        "logging.basicConfig(level='ERROR')\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import zlib\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "import matplotlib\n",
        "import random\n",
        "from google.colab import userdata"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvGHaYO2Y-mT"
      },
      "source": [
        "## Model Loading and Setup\n",
        "\n",
        "Functions for loading and configuring the language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-FHHlU9Y-mT"
      },
      "source": [
        "def load_model(name1, name2):\n",
        "    \"\"\"Load two models for comparison.\n",
        "\n",
        "    Args:\n",
        "        name1: Name/path of the first model\n",
        "        name2: Name/path of the second model\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (model1, model2, tokenizer1, tokenizer2)\n",
        "    \"\"\"\n",
        "    if \"davinci\" in name1:\n",
        "        model1 = None\n",
        "        tokenizer1 = None\n",
        "    else:\n",
        "        model1 = AutoModelForCausalLM.from_pretrained(name1, return_dict=True, device_map='auto')\n",
        "        model1.eval()\n",
        "        tokenizer1 = AutoTokenizer.from_pretrained(name1)\n",
        "\n",
        "    if \"davinci\" in name2:\n",
        "        model2 = None\n",
        "        tokenizer2 = None\n",
        "    else:\n",
        "        model2 = AutoModelForCausalLM.from_pretrained(name2, return_dict=True, device_map='auto')\n",
        "        model2.eval()\n",
        "        tokenizer2 = AutoTokenizer.from_pretrained(name2)\n",
        "    return model1, model2, tokenizer1, tokenizer2"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzzFB_B2Y-mT"
      },
      "source": [
        "## Perplexity Calculation\n",
        "\n",
        "Functions for calculating perplexity using both OpenAI and HuggingFace models."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmkjqo1NclDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ImImCy-Y-mT"
      },
      "source": [
        "def calculatePerplexity_gpt3(prompt, modelname):\n",
        "    \"\"\"Calculate perplexity using OpenAI's API.\"\"\"\n",
        "    prompt = prompt.replace('\\x00','')\n",
        "    responses = None\n",
        "    api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    # Map old model names to new ones\n",
        "    model_mapping = {\n",
        "        \"text-davinci-003\": \"gpt-3.5-turbo-instruct\",\n",
        "        \"text-davinci-002\": \"gpt-3.5-turbo-instruct\"\n",
        "    }\n",
        "    modelname = model_mapping.get(modelname, modelname)\n",
        "    while responses is None:\n",
        "        try:\n",
        "            responses = client.completions.create(\n",
        "                        model=modelname,\n",
        "                        prompt=prompt,\n",
        "                        max_tokens=1,\n",
        "                        temperature=1.0,\n",
        "                        logprobs=5,\n",
        "                        echo=True)\n",
        "        except openai.BadRequestError as e:\n",
        "            print(f\"OpenAI API Error: {str(e)}\")\n",
        "            if \"maximum context length\" in str(e).lower():\n",
        "                print(\"The input text is too long for the model's context window.\")\n",
        "            elif \"logprobs\" in str(e).lower():\n",
        "                print(\"The logprobs parameter is not supported or exceeds the maximum value of 5.\")\n",
        "            else:\n",
        "                print(\"Please check the OpenAI API documentation for more details.\")\n",
        "    data = responses.choices[0].logprobs\n",
        "    all_prob = [d for d in data.token_logprobs if d is not None]\n",
        "    p1 = np.exp(-np.mean(all_prob))\n",
        "    return p1, all_prob, np.mean(all_prob)\n",
        "\n",
        "def calculatePerplexity(sentence, model, tokenizer, gpu):\n",
        "    \"\"\"Calculate perplexity using HuggingFace models.\"\"\"\n",
        "\n",
        "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
        "    input_ids = input_ids.to(gpu)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    loss, logits = outputs[:2]\n",
        "\n",
        "    # Apply softmax to the logits to get probabilities\n",
        "    probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    all_prob = []\n",
        "    input_ids_processed = input_ids[0][1:]\n",
        "    for i, token_id in enumerate(input_ids_processed):\n",
        "        probability = probabilities[0, i, token_id].item()\n",
        "        all_prob.append(probability)\n",
        "    return torch.exp(loss).item(), all_prob, loss.item()"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTaaWJCNY-mU"
      },
      "source": [
        "## Inference and Evaluation\n",
        "\n",
        "Functions for performing inference and evaluating results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_inbF4z0Y-mU"
      },
      "source": [
        "def inference(model1, model2, tokenizer1, tokenizer2, text, ex, modelname1, modelname2):\n",
        "    \"\"\"Perform inference using both models and calculate metrics.\"\"\"\n",
        "    pred = {}\n",
        "\n",
        "    if \"davinci\" in modelname1:\n",
        "        p1, all_prob, p1_likelihood = calculatePerplexity_gpt3(text, modelname1)\n",
        "        p_lower, _, p_lower_likelihood = calculatePerplexity_gpt3(text.lower(), modelname1)\n",
        "    else:\n",
        "        p1, all_prob, p1_likelihood = calculatePerplexity(text, model1, tokenizer1, gpu=model1.device)\n",
        "        p_lower, _, p_lower_likelihood = calculatePerplexity(text.lower(), model1, tokenizer1, gpu=model1.device)\n",
        "\n",
        "    if \"davinci\" in modelname2:\n",
        "        p_ref, all_prob_ref, p_ref_likelihood = calculatePerplexity_gpt3(text, modelname2)\n",
        "    else:\n",
        "        p_ref, all_prob_ref, p_ref_likelihood = calculatePerplexity(text, model2, tokenizer2, gpu=model2.device)\n",
        "\n",
        "    # Calculate various metrics\n",
        "    pred[\"ppl\"] = p1\n",
        "    pred[\"ppl/Ref_ppl (calibrate PPL to the reference model)\"] = p1_likelihood-p_ref_likelihood\n",
        "    pred[\"ppl/lowercase_ppl\"] = -(np.log(p_lower) / np.log(p1)).item()\n",
        "    zlib_entropy = len(zlib.compress(bytes(text, 'utf-8')))\n",
        "    pred[\"ppl/zlib\"] = np.log(p1)/zlib_entropy\n",
        "\n",
        "    # Calculate min-k probabilities\n",
        "    for ratio in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
        "        k_length = int(len(all_prob)*ratio)\n",
        "        topk_prob = np.sort(all_prob)[:k_length]\n",
        "        pred[f\"Min_{ratio*100}% Prob\"] = -np.mean(topk_prob).item()\n",
        "\n",
        "    ex[\"pred\"] = pred\n",
        "    return ex\n",
        "\n",
        "def evaluate_data(test_data, model1, model2, tokenizer1, tokenizer2, col_name, modelname1, modelname2):\n",
        "    \"\"\"Evaluate data using both models.\"\"\"\n",
        "    print(f\"all data size: {len(test_data)}\")\n",
        "    all_output = []\n",
        "    for ex in tqdm(test_data):\n",
        "        text = ex[col_name]\n",
        "        new_ex = inference(model1, model2, tokenizer1, tokenizer2, text, ex, modelname1, modelname2)\n",
        "        all_output.append(new_ex)\n",
        "    return all_output"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4jv6GUsY-mU"
      },
      "source": [
        "## Visualization and Metrics\n",
        "\n",
        "Functions for plotting results and calculating metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acI-6VVeY-mU"
      },
      "source": [
        "def sweep(score, x):\n",
        "    \"\"\"Compute ROC curve and return metrics.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(x, -score)\n",
        "    acc = np.max(1-(fpr+(1-tpr))/2)\n",
        "    return fpr, tpr, auc(fpr, tpr), acc\n",
        "\n",
        "def do_plot(prediction, answers, sweep_fn=sweep, metric='auc', legend=\"\", output_dir=None):\n",
        "    \"\"\"Generate ROC curves and calculate metrics.\"\"\"\n",
        "    fpr, tpr, auc_score, acc = sweep_fn(np.array(prediction), np.array(answers, dtype=bool))\n",
        "    low = tpr[np.where(fpr<.05)[0][-1]]\n",
        "    print('Attack %s   AUC %.4f, Accuracy %.4f, TPR@5%%FPR of %.4f\\n'%(legend, auc_score, acc, low))\n",
        "\n",
        "    metric_text = ''\n",
        "    if metric == 'auc':\n",
        "        metric_text = 'auc=%.3f'%auc_score\n",
        "    elif metric == 'acc':\n",
        "        metric_text = 'acc=%.3f'%acc\n",
        "\n",
        "    plt.plot(fpr, tpr, label=legend+metric_text)\n",
        "    return legend, auc_score, acc, low\n",
        "\n",
        "def fig_fpr_tpr(all_output, output_dir):\n",
        "    \"\"\"Generate and save FPR-TPR plots.\"\"\"\n",
        "    print(\"output_dir\", output_dir)\n",
        "    answers = []\n",
        "    metric2predictions = defaultdict(list)\n",
        "    for ex in all_output:\n",
        "        answers.append(ex[\"label\"])\n",
        "        for metric in ex[\"pred\"].keys():\n",
        "            if (\"raw\" in metric) and (\"clf\" not in metric):\n",
        "                continue\n",
        "            metric2predictions[metric].append(ex[\"pred\"][metric])\n",
        "\n",
        "    plt.figure(figsize=(4,3))\n",
        "    with open(f\"{output_dir}/auc.txt\", \"w\") as f:\n",
        "        for metric, predictions in metric2predictions.items():\n",
        "            legend, auc_score, acc, low = do_plot(predictions, answers, legend=metric, metric='auc', output_dir=output_dir)\n",
        "            f.write('%s   AUC %.4f, Accuracy %.4f, TPR@0.1%%FPR of %.4f\\n'%(legend, auc_score, acc, low))\n",
        "\n",
        "    plt.semilogx()\n",
        "    plt.semilogy()\n",
        "    plt.xlim(1e-5,1)\n",
        "    plt.ylim(1e-5,1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.plot([0, 1], [0, 1], ls='--', color='gray')\n",
        "    plt.subplots_adjust(bottom=.18, left=.18, top=.96, right=.96)\n",
        "    plt.legend(fontsize=8)\n",
        "    plt.savefig(f\"{output_dir}/auc.png\")"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l4akTLfY-mV"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "Helper functions for data loading and manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_svnvJIDY-mV"
      },
      "source": [
        "def load_jsonl(input_path):\n",
        "    \"\"\"Load data from a JSONL file.\"\"\"\n",
        "    with open(input_path, 'r') as f:\n",
        "        data = [json.loads(line) for line in tqdm(f)]\n",
        "    random.seed(0)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "def dump_jsonl(data, path):\n",
        "    \"\"\"Save data to a JSONL file.\"\"\"\n",
        "    with open(path, 'w') as f:\n",
        "        for line in tqdm(data):\n",
        "            f.write(json.dumps(line) + \"\\n\")\n",
        "\n",
        "def read_jsonl(path):\n",
        "    \"\"\"Read data from a JSONL file.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        return [json.loads(line) for line in tqdm(f)]\n",
        "\n",
        "def convert_huggingface_data_to_list_dic(dataset):\n",
        "    \"\"\"Convert HuggingFace dataset to list of dictionaries.\"\"\"\n",
        "    all_data = []\n",
        "    for i in range(len(dataset)):\n",
        "        ex = dataset[i]\n",
        "        all_data.append(ex)\n",
        "    return all_data"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYXDamhOY-mV"
      },
      "source": [
        "## Example Usage\n",
        "\n",
        "Here's how to use the functions above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fJ0PXyKY-mV"
      },
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up output directory\n",
        "    output_dir = \"output\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load models\n",
        "    target_model = \"gpt-3.5-turbo-instruct\"\n",
        "    ref_model = \"huggyllama/llama-7b\"\n",
        "    model1, model2, tokenizer1, tokenizer2 = load_model(target_model, ref_model)\n",
        "\n",
        "    # Load data\n",
        "    dataset = load_dataset(\"swj0419/WikiMIA\", split=\"WikiMIA_length64\")\n",
        "    data = convert_huggingface_data_to_list_dic(dataset)\n",
        "\n",
        "    # Evaluate\n",
        "    all_output = evaluate_data(data, model1, model2, tokenizer1, tokenizer2, \"input\", target_model, ref_model)\n",
        "\n",
        "    # Plot results\n",
        "    fig_fpr_tpr(all_output, output_dir)"
      ],
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}