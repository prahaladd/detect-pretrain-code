{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Detecting Pretraining Data from Large Language Models\n",
        "\n",
        "This notebook implements the methods for detecting whether a piece of text was part of a language model's pretraining data. It includes functionality for:\n",
        "- Loading and preparing models\n",
        "- Calculating perplexity\n",
        "- Evaluating detection metrics\n",
        "- Visualizing results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "import logging\n",
        "logging.basicConfig(level='ERROR')\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from openai import OpenAI\n",
        "import torch\n",
        "import zlib\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import auc, roc_curve\n",
        "import matplotlib\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Loading and Setup\n",
        "\n",
        "Functions for loading and configuring the language models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def load_model(name1, name2):\n",
        "    \"\"\"Load two models for comparison.\n",
        "    \n",
        "    Args:\n",
        "        name1: Name/path of the first model\n",
        "        name2: Name/path of the second model\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (model1, model2, tokenizer1, tokenizer2)\n",
        "    \"\"\"\n",
        "    if \"davinci\" in name1:\n",
        "        model1 = None\n",
        "        tokenizer1 = None\n",
        "    else:\n",
        "        model1 = AutoModelForCausalLM.from_pretrained(name1, return_dict=True, device_map='auto')\n",
        "        model1.eval()\n",
        "        tokenizer1 = AutoTokenizer.from_pretrained(name1)\n",
        "\n",
        "    if \"davinci\" in name2:\n",
        "        model2 = None\n",
        "        tokenizer2 = None\n",
        "    else:\n",
        "        model2 = AutoModelForCausalLM.from_pretrained(name2, return_dict=True, device_map='auto')\n",
        "        model2.eval()\n",
        "        tokenizer2 = AutoTokenizer.from_pretrained(name2)\n",
        "    return model1, model2, tokenizer1, tokenizer2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perplexity Calculation\n",
        "\n",
        "Functions for calculating perplexity using both OpenAI and HuggingFace models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def calculatePerplexity_gpt3(prompt, modelname):\n",
        "    \"\"\"Calculate perplexity using OpenAI's API.\"\"\"\n",
        "    prompt = prompt.replace('\\x00','')\n",
        "    responses = None\n",
        "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    client = OpenAI(api_key=api_key)\n",
        "    # Map old model names to new ones\n",
        "    model_mapping = {\n",
        "        \"text-davinci-003\": \"gpt-3.5-turbo-instruct\",\n",
        "        \"text-davinci-002\": \"gpt-3.5-turbo-instruct\"\n",
        "    }\n",
        "    modelname = model_mapping.get(modelname, modelname)\n",
        "    while responses is None:\n",
        "        try:\n",
        "            responses = client.completions.create(\n",
        "                        model=modelname, \n",
        "                        prompt=prompt,\n",
        "                        max_tokens=1,\n",
        "                        temperature=1.0,\n",
        "                        logprobs=5,\n",
        "                        echo=True)\n",
        "        except openai.BadRequestError as e:\n",
        "            print(f\"OpenAI API Error: {str(e)}\")\n",
        "            if \"maximum context length\" in str(e).lower():\n",
        "                print(\"The input text is too long for the model's context window.\")\n",
        "            elif \"logprobs\" in str(e).lower():\n",
        "                print(\"The logprobs parameter is not supported or exceeds the maximum value of 5.\")\n",
        "            else:\n",
        "                print(\"Please check the OpenAI API documentation for more details.\")\n",
        "    data = responses.choices[0].logprobs\n",
        "    all_prob = [d for d in data.token_logprobs if d is not None]\n",
        "    p1 = np.exp(-np.mean(all_prob))\n",
        "    return p1, all_prob, np.mean(all_prob)\n",
        "\n",
        "def calculatePerplexity(sentence, model, tokenizer, gpu):\n",
        "    \"\"\"Calculate perplexity using HuggingFace models.\"\"\"\n",
        "    # Set the default device to mps\n",
        "    torch.set_default_device(\"mps\")\n",
        "    input_ids = torch.tensor(tokenizer.encode(sentence)).unsqueeze(0)\n",
        "    input_ids = input_ids.to(\"mps\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=input_ids)\n",
        "    loss, logits = outputs[:2]\n",
        "    \n",
        "    # Apply softmax to the logits to get probabilities\n",
        "    probabilities = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "    all_prob = []\n",
        "    input_ids_processed = input_ids[0][1:]\n",
        "    for i, token_id in enumerate(input_ids_processed):\n",
        "        probability = probabilities[0, i, token_id].item()\n",
        "        all_prob.append(probability)\n",
        "    return torch.exp(loss).item(), all_prob, loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference and Evaluation\n",
        "\n",
        "Functions for performing inference and evaluating results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def inference(model1, model2, tokenizer1, tokenizer2, text, ex, modelname1, modelname2):\n",
        "    \"\"\"Perform inference using both models and calculate metrics.\"\"\"\n",
        "    pred = {}\n",
        "\n",
        "    if \"davinci\" in modelname1:\n",
        "        p1, all_prob, p1_likelihood = calculatePerplexity_gpt3(text, modelname1) \n",
        "        p_lower, _, p_lower_likelihood = calculatePerplexity_gpt3(text.lower(), modelname1)\n",
        "    else:\n",
        "        p1, all_prob, p1_likelihood = calculatePerplexity(text, model1, tokenizer1, gpu=model1.device)\n",
        "        p_lower, _, p_lower_likelihood = calculatePerplexity(text.lower(), model1, tokenizer1, gpu=model1.device)\n",
        "\n",
        "    if \"davinci\" in modelname2:\n",
        "        p_ref, all_prob_ref, p_ref_likelihood = calculatePerplexity_gpt3(text, modelname2)\n",
        "    else:\n",
        "        p_ref, all_prob_ref, p_ref_likelihood = calculatePerplexity(text, model2, tokenizer2, gpu=model2.device)\n",
        "   \n",
        "    # Calculate various metrics\n",
        "    pred[\"ppl\"] = p1\n",
        "    pred[\"ppl/Ref_ppl (calibrate PPL to the reference model)\"] = p1_likelihood-p_ref_likelihood\n",
        "    pred[\"ppl/lowercase_ppl\"] = -(np.log(p_lower) / np.log(p1)).item()\n",
        "    zlib_entropy = len(zlib.compress(bytes(text, 'utf-8')))\n",
        "    pred[\"ppl/zlib\"] = np.log(p1)/zlib_entropy\n",
        "    \n",
        "    # Calculate min-k probabilities\n",
        "    for ratio in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]:\n",
        "        k_length = int(len(all_prob)*ratio)\n",
        "        topk_prob = np.sort(all_prob)[:k_length]\n",
        "        pred[f\"Min_{ratio*100}% Prob\"] = -np.mean(topk_prob).item()\n",
        "\n",
        "    ex[\"pred\"] = pred\n",
        "    return ex\n",
        "\n",
        "def evaluate_data(test_data, model1, model2, tokenizer1, tokenizer2, col_name, modelname1, modelname2):\n",
        "    \"\"\"Evaluate data using both models.\"\"\"\n",
        "    print(f\"all data size: {len(test_data)}\")\n",
        "    all_output = []\n",
        "    for ex in tqdm(test_data): \n",
        "        text = ex[col_name]\n",
        "        new_ex = inference(model1, model2, tokenizer1, tokenizer2, text, ex, modelname1, modelname2)\n",
        "        all_output.append(new_ex)\n",
        "    return all_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization and Metrics\n",
        "\n",
        "Functions for plotting results and calculating metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def sweep(score, x):\n",
        "    \"\"\"Compute ROC curve and return metrics.\"\"\"\n",
        "    fpr, tpr, _ = roc_curve(x, -score)\n",
        "    acc = np.max(1-(fpr+(1-tpr))/2)\n",
        "    return fpr, tpr, auc(fpr, tpr), acc\n",
        "\n",
        "def do_plot(prediction, answers, sweep_fn=sweep, metric='auc', legend=\"\", output_dir=None):\n",
        "    \"\"\"Generate ROC curves and calculate metrics.\"\"\"\n",
        "    fpr, tpr, auc_score, acc = sweep_fn(np.array(prediction), np.array(answers, dtype=bool))\n",
        "    low = tpr[np.where(fpr<.05)[0][-1]]\n",
        "    print('Attack %s   AUC %.4f, Accuracy %.4f, TPR@5%%FPR of %.4f\\n'%(legend, auc_score, acc, low))\n",
        "\n",
        "    metric_text = ''\n",
        "    if metric == 'auc':\n",
        "        metric_text = 'auc=%.3f'%auc_score\n",
        "    elif metric == 'acc':\n",
        "        metric_text = 'acc=%.3f'%acc\n",
        "\n",
        "    plt.plot(fpr, tpr, label=legend+metric_text)\n",
        "    return legend, auc_score, acc, low\n",
        "\n",
        "def fig_fpr_tpr(all_output, output_dir):\n",
        "    \"\"\"Generate and save FPR-TPR plots.\"\"\"\n",
        "    print(\"output_dir\", output_dir)\n",
        "    answers = []\n",
        "    metric2predictions = defaultdict(list)\n",
        "    for ex in all_output:\n",
        "        answers.append(ex[\"label\"])\n",
        "        for metric in ex[\"pred\"].keys():\n",
        "            if (\"raw\" in metric) and (\"clf\" not in metric):\n",
        "                continue\n",
        "            metric2predictions[metric].append(ex[\"pred\"][metric])\n",
        "    \n",
        "    plt.figure(figsize=(4,3))\n",
        "    with open(f\"{output_dir}/auc.txt\", \"w\") as f:\n",
        "        for metric, predictions in metric2predictions.items():\n",
        "            legend, auc_score, acc, low = do_plot(predictions, answers, legend=metric, metric='auc', output_dir=output_dir)\n",
        "            f.write('%s   AUC %.4f, Accuracy %.4f, TPR@0.1%%FPR of %.4f\\n'%(legend, auc_score, acc, low))\n",
        "\n",
        "    plt.semilogx()\n",
        "    plt.semilogy()\n",
        "    plt.xlim(1e-5,1)\n",
        "    plt.ylim(1e-5,1)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.plot([0, 1], [0, 1], ls='--', color='gray')\n",
        "    plt.subplots_adjust(bottom=.18, left=.18, top=.96, right=.96)\n",
        "    plt.legend(fontsize=8)\n",
        "    plt.savefig(f\"{output_dir}/auc.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utility Functions\n",
        "\n",
        "Helper functions for data loading and manipulation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "def load_jsonl(input_path):\n",
        "    \"\"\"Load data from a JSONL file.\"\"\"\n",
        "    with open(input_path, 'r') as f:\n",
        "        data = [json.loads(line) for line in tqdm(f)]\n",
        "    random.seed(0)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "def dump_jsonl(data, path):\n",
        "    \"\"\"Save data to a JSONL file.\"\"\"\n",
        "    with open(path, 'w') as f:\n",
        "        for line in tqdm(data):\n",
        "            f.write(json.dumps(line) + \"\\n\")\n",
        "\n",
        "def read_jsonl(path):\n",
        "    \"\"\"Read data from a JSONL file.\"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        return [json.loads(line) for line in tqdm(f)]\n",
        "\n",
        "def convert_huggingface_data_to_list_dic(dataset):\n",
        "    \"\"\"Convert HuggingFace dataset to list of dictionaries.\"\"\"\n",
        "    all_data = []\n",
        "    for i in range(len(dataset)):\n",
        "        ex = dataset[i]\n",
        "        all_data.append(ex)\n",
        "    return all_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Usage\n",
        "\n",
        "Here's how to use the functions above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set up output directory\n",
        "    output_dir = \"output\"\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # Load models\n",
        "    target_model = \"gpt-3.5-turbo-instruct\"\n",
        "    ref_model = \"huggyllama/llama-7b\"\n",
        "    model1, model2, tokenizer1, tokenizer2 = load_model(target_model, ref_model)\n",
        "    \n",
        "    # Load data\n",
        "    dataset = load_dataset(\"swj0419/WikiMIA\", split=\"WikiMIA_length64\")\n",
        "    data = convert_huggingface_data_to_list_dic(dataset)\n",
        "    \n",
        "    # Evaluate\n",
        "    all_output = evaluate_data(data, model1, model2, tokenizer1, tokenizer2, \"input\", target_model, ref_model)\n",
        "    \n",
        "    # Plot results\n",
        "    fig_fpr_tpr(all_output, output_dir)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
